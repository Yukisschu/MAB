{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2beffb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/giulia/Documents/BDS/Reinforcement learning/MAB assignments/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # Numerical computing primitives for vectorized linear algebra.\n",
    "import pandas as pd  # Tabular data loading and preprocessing utilities.\n",
    "from sklearn.feature_extraction import FeatureHasher  # Feature hashing for high-cardinality categorical context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbafce9",
   "metadata": {},
   "source": [
    "### Data Loading and Cleaning  \n",
    "The ZOZOTOWN contextual bandit dataset is loaded from a CSV file and copied to preserve the original data. Column names are standardized to ensure consistent access across the pipeline. The presence of required variables is validated, and key columns are cast to appropriate numeric types to ensure well-defined computations.\n",
    "\n",
    "### Context Feature Processing  \n",
    "Contextual information is extracted from user-related variables with prefix `user_feature_`. These variables represent anonymized, high-cardinality categorical attributes and therefore cannot be directly interpreted as numerical features.\n",
    "\n",
    "### Feature Hashing  \n",
    "Feature hashing is applied to transform categorical context features into a fixed-dimensional numeric representation. Each context vector is mapped into $\\mathbb{R}^d$ with $d = 64$ using a signed hashing scheme to mitigate collision bias. The resulting matrix $X \\in \\mathbb{R}^{N \\times d}$ serves as the numerical context input for the contextual bandit algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed context matrix shape: (1356670, 64)\n",
      "Sample hashed context row (first 5 features): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load + minimal cleanup\n",
    "# ------------------------------------------------------------\n",
    "dfZozo_80 = pd.read_csv(\"zozo_Context_80items.csv\")  # Load the ZOZO contextual dataset containing 80 items.\n",
    "\n",
    "df = dfZozo_80.copy()  # Create a defensive copy to avoid mutating the original DataFrame.\n",
    "df.columns = (  # Standardize column names for consistent downstream access.\n",
    "    df.columns.astype(str)  # Ensure the column index is string-typed.\n",
    "    .str.strip()  # Remove leading and trailing whitespace.\n",
    "    .str.lower()  # Convert to lowercase for case-insensitive referencing.\n",
    "    .str.replace(r\"\\s+\", \"_\", regex=True)  # Replace runs of whitespace with underscores.\n",
    "    .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)  # Remove non-alphanumeric/underscore characters.\n",
    ")\n",
    "\n",
    "required = [\"item_id\", \"position\", \"click\", \"propensity_score\"]  # Define the minimal set of required columns.\n",
    "missing = [c for c in required if c not in df.columns]  # Identify missing required columns after normalization.\n",
    "if missing:  # Validate schema conformance before continuing.\n",
    "    raise ValueError(  # Raise a clear error describing the mismatch.\n",
    "        f\"Missing required columns after cleanup: {missing}. \"\n",
    "        f\"Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "ctx_cols = [c for c in df.columns if c.startswith(\"user_feature_\")]  # Select contextual user feature columns.\n",
    "if not ctx_cols:  # Ensure context features exist in the dataset.\n",
    "    raise ValueError(\"No context columns found. Expected columns like user_feature_0..3\")  # Fail fast on schema mismatch.\n",
    "\n",
    "df[\"click\"] = df[\"click\"].astype(int)  # Cast reward to integer-coded binary labels.\n",
    "df[\"propensity_score\"] = df[\"propensity_score\"].astype(float)  # Cast logged propensities to floating point.\n",
    "df[\"item_id\"] = df[\"item_id\"].astype(int)  # Cast item identifiers to integer.\n",
    "df[\"position\"] = df[\"position\"].astype(int)  # Cast position identifiers to integer.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Context preprocessing via feature hashing\n",
    "# ------------------------------------------------------------\n",
    "hashed_cols = ctx_cols  # Treat user_feature_* columns as hashed categorical features.\n",
    "hashed_records = (  # Convert each row into a mapping suitable for FeatureHasher.\n",
    "    df[hashed_cols]\n",
    "    .astype(str)  # Ensure all hashed values are represented as strings.\n",
    "    .to_dict(orient=\"records\")  # Convert to a list of row-wise dictionaries.\n",
    ")\n",
    "\n",
    "hash_dim = 64  # Set the target dimensionality for the hashed feature space.\n",
    "hasher = FeatureHasher(  # Instantiate a feature hasher for scalable categorical encoding.\n",
    "    n_features=hash_dim,  # Fix the output dimension of the feature hashing transform.\n",
    "    input_type=\"dict\",  # Indicate that each sample is represented as a dictionary.\n",
    "    alternate_sign=True  # Use signed hashing to reduce collisions bias.\n",
    ")\n",
    "\n",
    "X = hasher.transform(hashed_records).toarray().astype(float)  # Produce a dense numeric context matrix for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfZozo_80.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "233e69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashed context matrix shape: (1356670, 64)\n",
      "Sample hashed context row (first 5 features): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hashed context matrix shape: {X.shape}\")  # Log the shape of the resulting context matrix.\n",
    "print(f\"Sample hashed context row (first 5 features): {X[0, :20]}\")  # Display a sample hashed context vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9c7de",
   "metadata": {},
   "source": [
    "### Utility Functions  \n",
    "\n",
    "The logistic sigmoid function maps a real-valued score to a probability in the interval $(0,1)$.  \n",
    "For an input $z \\in \\mathbb{R}$, the sigmoid function is defined as\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + \\exp(-z)} .\n",
    "$$\n",
    "For numerical stability, the input $z$ is clipped to a bounded range before applying the exponential function.\n",
    "\n",
    "An action indexing function maps each itemâ€“position pair to a unique scalar action identifier.  \n",
    "Given an item index $i \\in \\{0,\\dots,79\\}$ and a position index $p \\in \\{0,1,2\\}$, the action ID is defined as\n",
    "$$\n",
    "a = i \\times n_{\\text{positions}} + p ,\n",
    "$$\n",
    "where $n_{\\text{positions}} = 3$.\n",
    "\n",
    "As an illustrative example:\n",
    "- for item $i = 0$ and positions $p = 0,1,2$,\n",
    "$$\n",
    "a = 0 \\times 3 + p \\in \\{0,1,2\\},\n",
    "$$\n",
    "- for item $i = 1$,\n",
    "$$\n",
    "a = 1 \\times 3 + p \\in \\{3,4,5\\},\n",
    "$$\n",
    "- for item $i = 79$,\n",
    "$$\n",
    "a = 79 \\times 3 + p \\in \\{237,238,239\\}.\n",
    "$$\n",
    "\n",
    "Therefore, the action index satisfies\n",
    "$$\n",
    "a \\in \\{0,\\dots,239\\},\n",
    "$$\n",
    "and the total number of actions is $80 \\times 3 = 240$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5933ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Utilities\n",
    "# ------------------------------------------------------------\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:  # Define the logistic sigmoid function.\n",
    "    z = np.clip(z, -35, 35)  # Clip logits to prevent numerical overflow in exp().\n",
    "    return 1.0 / (1.0 + np.exp(-z))  # Compute elementwise sigmoid.\n",
    "\n",
    "def make_action_id(item_id: np.ndarray, position: np.ndarray, n_positions: int = 3) -> np.ndarray:  # Map (item,position) to a single action index.\n",
    "    item0 = item_id.copy()  # Copy to avoid side effects on the input array.\n",
    "    if item0.min() == 1:  # Normalize 1-based item indexing to 0-based indexing if required.\n",
    "        item0 = item0 - 1  # Convert item IDs from {1..80} to {0..79}.\n",
    "    pos0 = position - 1  # Convert position IDs from {1,2,3} to {0,1,2}.\n",
    "    return item0 * n_positions + pos0  # Compute unique action IDs in [0, 80*3-1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1844b",
   "metadata": {},
   "source": [
    "### Logistic Thompson Sampling (LogTS)\n",
    "\n",
    "Logistic Thompson Sampling is implemented with an action-specific diagonal Gaussian posterior over the logistic regression parameters.  \n",
    "For each action $a \\in \\{0,\\dots,K-1\\}$, the posterior approximation is\n",
    "$$\n",
    "\\theta_a \\sim \\mathcal{N}\\!\\left(m_a,\\;\\mathrm{diag}(q_a)^{-1}\\right),\n",
    "$$\n",
    "where $m_a \\in \\mathbb{R}^d$ is the posterior mean and $q_a \\in \\mathbb{R}^d$ is the diagonal precision vector.\n",
    "\n",
    "An optional intercept term is included by augmenting the context vector $x \\in \\mathbb{R}^{d-1}$ into\n",
    "$$\n",
    "\\tilde{x} = \\begin{bmatrix} x \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{d}.\n",
    "$$\n",
    "\n",
    "At each round, LogTS performs posterior sampling for action selection. A parameter vector is sampled for each action,\n",
    "$$\n",
    "\\tilde{\\theta}_a \\sim \\mathcal{N}\\!\\left(m_a,\\;\\mathrm{diag}(q_a)^{-1}\\right),\n",
    "$$\n",
    "and the click probability under the sampled parameters is computed as\n",
    "$$\n",
    "p_a = \\sigma(\\tilde{\\theta}_a^\\top \\tilde{x}).\n",
    "$$\n",
    "The selected action is\n",
    "$$\n",
    "\\hat{a} = \\arg\\max_{a} \\; p_a.\n",
    "$$\n",
    "\n",
    "Only partial feedback is available in logged bandit data. The posterior update is applied only when the selected action matches the logged action $a_i$.\n",
    "\n",
    "The posterior mean update is implemented as a one-step Newton approximation to the MAP estimate under a diagonal Gaussian prior centered at the current mean.  \n",
    "Let $w$ denote the current iterate, and define\n",
    "$$\n",
    "p = \\sigma(w^\\top \\tilde{x}).\n",
    "$$\n",
    "The gradient of the negative log-posterior is\n",
    "$$\n",
    "g = q \\odot (w - m_0) + (p - r)\\tilde{x},\n",
    "$$\n",
    "where $m_0$ is the current mean, $q$ is the current diagonal precision, and $\\odot$ denotes elementwise multiplication.  \n",
    "The Hessian has the form\n",
    "$$\n",
    "H = \\mathrm{diag}(q) + p(1-p)\\tilde{x}\\tilde{x}^\\top,\n",
    "$$\n",
    "and the Newton step updates $w \\leftarrow w - H^{-1}g$.\n",
    "\n",
    "After updating the mean, the diagonal precision is updated using a Laplace-style curvature approximation:\n",
    "$$\n",
    "q \\leftarrow q + \\tilde{x}^{\\odot 2}\\, p(1-p),\n",
    "$$\n",
    "where $\\tilde{x}^{\\odot 2}$ denotes elementwise squaring.\n",
    "\n",
    "For off-policy evaluation, the action selection probability $\\pi_e(a\\mid x)$ is approximated by Monte Carlo posterior sampling.  \n",
    "For $M$ samples, the policy probability is estimated as\n",
    "$$\n",
    "\\widehat{\\pi}_e(a\\mid x) = \\frac{1}{M}\\sum_{m=1}^{M} \\mathbf{1}\\!\\left[a = \\arg\\max_{a'} \\sigma\\!\\left(\\tilde{\\theta}^{(m)}_{a'}{}^\\top \\tilde{x}\\right)\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06a4721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# LogTS implementation (diagonal Gaussian posterior per action)\n",
    "# Posterior: theta_a ~ N(m_a, diag(1/q_a))\n",
    "# Online update (batch size 1) with partial feedback:\n",
    "# - Sample theta_a for each action.\n",
    "# - Select action maximizing sigmoid(theta_a^T x).\n",
    "# - Update only when the selected action matches the logged action.\n",
    "# - Apply a one-step Newton update toward the MAP estimate under a diagonal Gaussian prior.\n",
    "# - Update diagonal precision via p(1-p) x^2.\n",
    "# ------------------------------------------------------------\n",
    "class LogTS:  # Define a Logistic Thompson Sampling policy with diagonal Gaussian posteriors.\n",
    "    def __init__(  # Initialize model state and prior parameters.\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        context_dim: int,\n",
    "        lam: float = 1.0,\n",
    "        seed: int = 42,\n",
    "        add_intercept: bool = True,\n",
    "    ):\n",
    "        self.n_actions = n_actions  # Store number of discrete actions.\n",
    "        self.add_intercept = add_intercept  # Indicate whether to append an intercept to context vectors.\n",
    "        self.rng = np.random.default_rng(seed)  # Initialize a reproducible random number generator.\n",
    "\n",
    "        self.d = context_dim + (1 if add_intercept else 0)  # Determine parameter dimension, including intercept if enabled.\n",
    "\n",
    "        self.m = np.zeros((n_actions, self.d), dtype=float)  # Initialize posterior means for all actions.\n",
    "        self.q = np.full((n_actions, self.d), lam, dtype=float)  # Initialize diagonal precisions for all actions.\n",
    "\n",
    "    def _augment_x(self, x: np.ndarray) -> np.ndarray:  # Augment a context vector with an intercept term if required.\n",
    "        if not self.add_intercept:  # Skip augmentation when intercept is disabled.\n",
    "            return x  # Return the context unchanged.\n",
    "        return np.concatenate([x, np.array([1.0])], axis=0)  # Append a constant intercept feature.\n",
    "\n",
    "    def sample_thetas(self) -> np.ndarray:  # Sample parameter vectors for all actions from their posteriors.\n",
    "        std = 1.0 / np.sqrt(self.q)  # Convert diagonal precision to standard deviation.\n",
    "        return self.m + self.rng.normal(size=self.m.shape) * std  # Draw Normal samples for all actions.\n",
    "\n",
    "    def choose_action(self, x: np.ndarray) -> int:  # Select an action using posterior sampling and logistic scoring.\n",
    "        x_aug = self._augment_x(x)  # Construct the augmented context vector.\n",
    "        thetas = self.sample_thetas()  # Draw one parameter sample per action.\n",
    "        logits = thetas @ x_aug  # Compute sampled logits for each action.\n",
    "        probs = sigmoid(logits)  # Convert logits to click probabilities.\n",
    "        return int(np.argmax(probs))  # Choose the action with the highest sampled probability.\n",
    "\n",
    "    def update_selected_action(self, a: int, x: np.ndarray, r: int, newton_steps: int = 1):  # Update the posterior of a selected action using one-step online Bayesian logistic regression.\n",
    "        x_aug = self._augment_x(x)  # Construct the augmented context vector.\n",
    "\n",
    "        m0 = self.m[a].copy()  # Store the prior mean (current posterior mean) for action a.\n",
    "        q = self.q[a]  # Retrieve diagonal precision vector for action a.\n",
    "        w = m0.copy()  # Initialize the Newton iterate at the prior mean.\n",
    "\n",
    "        for _ in range(newton_steps):  # Perform a small number of Newton steps for a MAP approximation.\n",
    "            p = float(sigmoid(np.array([w @ x_aug]))[0])  # Evaluate sigmoid at the current iterate.\n",
    "\n",
    "            g = q * (w - m0) + (p - r) * x_aug  # Compute gradient of negative log-posterior under diagonal Gaussian prior.\n",
    "\n",
    "            s = p * (1.0 - p)  # Compute curvature term for the logistic likelihood.\n",
    "            D_inv = 1.0 / q  # Compute inverse of diagonal precision for Sherman-Morrison update.\n",
    "            Dx = D_inv * x_aug  # Compute D^{-1} x for the rank-one correction.\n",
    "            denom = 1.0 + s * float(x_aug @ Dx)  # Compute scalar denominator 1 + s x^T D^{-1} x.\n",
    "\n",
    "            Dinv_g = D_inv * g  # Compute D^{-1} g.\n",
    "            corr = (s / denom) * (Dx * float(x_aug @ Dinv_g))  # Compute Sherman-Morrison correction term.\n",
    "            delta = Dinv_g - corr  # Solve for Newton step delta = (D + s x x^T)^{-1} g.\n",
    "\n",
    "            w = w - delta  # Apply Newton update to the iterate.\n",
    "\n",
    "        self.m[a] = w  # Commit updated posterior mean for action a.\n",
    "\n",
    "        p_new = float(sigmoid(np.array([w @ x_aug]))[0])  # Compute probability at the updated mean.\n",
    "        s_new = p_new * (1.0 - p_new)  # Compute logistic curvature at the updated mean.\n",
    "        self.q[a] = q + (x_aug ** 2) * s_new  # Update diagonal precision using a Laplace-style approximation.\n",
    "\n",
    "    def fit_online(self, X: np.ndarray, A_logged: np.ndarray, R: np.ndarray, n_rounds: int = None):  # Simulate LogTS over a sequence of contexts with partial feedback updates.\n",
    "        n = X.shape[0] if n_rounds is None else min(n_rounds, X.shape[0])  # Determine the number of rounds to simulate.\n",
    "        chosen = np.empty(n, dtype=int)  # Allocate output array for chosen actions.\n",
    "\n",
    "        for i in range(n):  # Iterate over interaction rounds.\n",
    "            a_hat = self.choose_action(X[i])  # Select an action under LogTS for context i.\n",
    "            chosen[i] = a_hat  # Store the selected action.\n",
    "\n",
    "            if a_hat == A_logged[i]:  # Check whether the selected action matches the logged action.\n",
    "                self.update_selected_action(a_hat, X[i], int(R[i]), newton_steps=1)  # Update posterior using observed reward for the matching action.\n",
    "\n",
    "        return chosen  # Return the counterfactual action log produced by LogTS.\n",
    "\n",
    "    def action_probs_mc(self, x: np.ndarray, n_mc: int = 200) -> np.ndarray:  # Approximate pi_e(a|x) by Monte Carlo posterior sampling with argmax selection.\n",
    "        x_aug = self._augment_x(x)  # Construct the augmented context vector.\n",
    "        counts = np.zeros(self.n_actions, dtype=float)  # Initialize action selection counts.\n",
    "\n",
    "        std = 1.0 / np.sqrt(self.q)  # Convert diagonal precision to standard deviation for sampling.\n",
    "        for _ in range(n_mc):  # Repeat sampling to approximate selection probabilities.\n",
    "            thetas = self.m + self.rng.normal(size=self.m.shape) * std  # Sample one parameter vector per action.\n",
    "            a = int(np.argmax(sigmoid(thetas @ x_aug)))  # Select the action with highest sampled click probability.\n",
    "            counts[a] += 1.0  # Increment selection count for the chosen action.\n",
    "\n",
    "        return counts / n_mc  # Normalize counts to obtain an empirical action probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6a1d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# OPE estimators: IPW and SNIPW + bootstrap CI\n",
    "# ------------------------------------------------------------\n",
    "def ipw(pi_e: np.ndarray, pi_b: np.ndarray, r: np.ndarray) -> float:  # Compute the inverse probability weighting estimate of the policy value.\n",
    "    w = pi_e / np.clip(pi_b, 1e-12, None)  # Compute importance weights with numerical protection against division by zero.\n",
    "    return float(np.mean(w * r))  # Return the sample mean of weighted rewards.\n",
    "\n",
    "def snipw(pi_e: np.ndarray, pi_b: np.ndarray, r: np.ndarray) -> float:  # Compute the self-normalized IPW estimate of the policy value.\n",
    "    w = pi_e / np.clip(pi_b, 1e-12, None)  # Compute importance weights with numerical protection against division by zero.\n",
    "    num = float(np.sum(w * r))  # Compute weighted reward sum.\n",
    "    den = float(np.sum(w))  # Compute weight sum for normalization.\n",
    "    return num / den if den > 0 else 0.0  # Return normalized estimate, guarding against a zero denominator.\n",
    "\n",
    "def bootstrap_ci(values: np.ndarray, alpha: float = 0.05):  # Compute a two-sided percentile bootstrap confidence interval.\n",
    "    lo = float(np.quantile(values, alpha / 2.0))  # Compute lower percentile bound.\n",
    "    hi = float(np.quantile(values, 1.0 - alpha / 2.0))  # Compute upper percentile bound.\n",
    "    return lo, hi  # Return the confidence interval endpoints.\n",
    "\n",
    "def ope_with_bootstrap(pi_e: np.ndarray, pi_b: np.ndarray, r: np.ndarray, B: int = 100, seed: int = 7):  # Evaluate IPW and SNIPW with bootstrap confidence intervals.\n",
    "    rng = np.random.default_rng(seed)  # Initialize reproducible bootstrap sampling.\n",
    "    n = len(r)  # Determine sample size.\n",
    "\n",
    "    ipw_vals = np.empty(B, dtype=float)  # Allocate bootstrap storage for IPW estimates.\n",
    "    snipw_vals = np.empty(B, dtype=float)  # Allocate bootstrap storage for SNIPW estimates.\n",
    "\n",
    "    for b in range(B):  # Perform bootstrap resampling.\n",
    "        idx = rng.integers(0, n, size=n)  # Draw bootstrap indices with replacement.\n",
    "        ipw_vals[b] = ipw(pi_e[idx], pi_b[idx], r[idx])  # Compute IPW estimate on bootstrap sample.\n",
    "        snipw_vals[b] = snipw(pi_e[idx], pi_b[idx], r[idx])  # Compute SNIPW estimate on bootstrap sample.\n",
    "\n",
    "    return {  # Return point estimates and percentile confidence intervals.\n",
    "        \"ipw\": float(ipw(pi_e, pi_b, r)),  # Compute IPW point estimate on full data.\n",
    "        \"ipw_ci95\": bootstrap_ci(ipw_vals, alpha=0.05),  # Compute 95% bootstrap CI for IPW.\n",
    "        \"snipw\": float(snipw(pi_e, pi_b, r)),  # Compute SNIPW point estimate on full data.\n",
    "        \"snipw_ci95\": bootstrap_ci(snipw_vals, alpha=0.05),  # Compute 95% bootstrap CI for SNIPW.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fcc636",
   "metadata": {},
   "source": [
    "### End-to-End Pipeline: LogTS Simulation and Off-Policy Evaluation\n",
    "\n",
    "The logged action $a_i$ is constructed from the item and position fields by mapping each $(\\text{item\\_id}, \\text{position})$ pair to a unique action index. The logged binary reward is extracted as $r_i \\in \\{0,1\\}$, and the behavior policy propensity $\\pi_b(a_i\\mid x_i)$ is read from the propensity score column.\n",
    "\n",
    "The action space size is defined as\n",
    "$$\n",
    "K = 80 \\times 3 = 240,\n",
    "$$\n",
    "corresponding to 80 items and 3 display positions.\n",
    "\n",
    "A Logistic Thompson Sampling policy is instantiated with context dimension $d$ equal to the number of hashed context features, and with a diagonal Gaussian prior precision controlled by $\\lambda$. The policy is then simulated sequentially over the logged contexts $\\{x_i\\}_{i=1}^{N}$ using partial-feedback updates, i.e., posterior updates are applied only when the selected action matches the logged action.\n",
    "\n",
    "For off-policy evaluation, the evaluation policy probability of the logged action, $\\pi_e(a_i\\mid x_i)$, is required for each interaction. Since LogTS defines $\\pi_e$ implicitly through posterior sampling and an argmax decision rule, $\\pi_e(a_i\\mid x_i)$ is approximated via Monte Carlo:\n",
    "$$\n",
    "\\widehat{\\pi}_e(a_i\\mid x_i) = \\frac{1}{M}\\sum_{m=1}^{M} \n",
    "\\mathbf{1}\\!\\left[a_i = \\arg\\max_{a} \\sigma\\!\\left(\\tilde{\\theta}^{(m)}_{a}{}^\\top \\tilde{x}_i\\right)\\right],\n",
    "$$\n",
    "where $M$ is the number of Monte Carlo samples, $\\tilde{\\theta}^{(m)}_a$ is a posterior sample for action $a$, and $\\tilde{x}_i$ denotes the context vector augmented with an intercept if applicable.\n",
    "\n",
    "Given $\\widehat{\\pi}_e(a_i\\mid x_i)$, the policy value is estimated using importance sampling estimators. The IPW estimator is\n",
    "$$\n",
    "\\widehat{V}_{\\mathrm{IPW}}(\\pi_e)\n",
    "= \\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\frac{\\widehat{\\pi}_e(a_i\\mid x_i)}{\\pi_b(a_i\\mid x_i)}\\, r_i,\n",
    "$$\n",
    "and the SNIPW estimator is\n",
    "$$\n",
    "\\widehat{V}_{\\mathrm{SNIPW}}(\\pi_e)\n",
    "=\n",
    "\\frac{\\sum_{i=1}^{N}\\frac{\\widehat{\\pi}_e(a_i\\mid x_i)}{\\pi_b(a_i\\mid x_i)}\\, r_i}\n",
    "{\\sum_{i=1}^{N}\\frac{\\widehat{\\pi}_e(a_i\\mid x_i)}{\\pi_b(a_i\\mid x_i)}}.\n",
    "$$\n",
    "\n",
    "Uncertainty is quantified using non-parametric bootstrap with $B=100$ resamples, producing percentile-based $95\\%$ confidence intervals for both IPW and SNIPW estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8af424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogTS simulation finished.\n",
      "pi_e(a_i|x_i) estimated for all rows.\n",
      "OPE results (reduced MC/bootstrap):\n",
      "{'ipw': 0.0005425048095704924, 'ipw_ci95': (0.0004009818157694944, 0.000674003257977253), 'snipw': 0.00275498592561538, 'snipw_ci95': (0.002049045604013863, 0.0034229662089410758)}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# End-to-end (full data): simulate LogTS, compute OPE with reduced MC/bootstrap\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "M_fast = 5     # Monte Carlo samples per context (reduce from 200)\n",
    "B_fast = 20    # bootstrap resamples (reduce from 100)\n",
    "\n",
    "# Logged arrays over the full dataset\n",
    "A_logged = make_action_id(\n",
    "    df[\"item_id\"].to_numpy(),\n",
    "    df[\"position\"].to_numpy(),\n",
    "    n_positions=3\n",
    ")\n",
    "R = df[\"click\"].to_numpy().astype(int)\n",
    "pi_b = df[\"propensity_score\"].to_numpy().astype(float)\n",
    "\n",
    "K = 80 * 3\n",
    "\n",
    "# Simulate LogTS over the full dataset (partial-feedback updates)\n",
    "logts = LogTS(\n",
    "    n_actions=K,\n",
    "    context_dim=X.shape[1],\n",
    "    lam=1.0,\n",
    "    seed=123,\n",
    "    add_intercept=True\n",
    ")\n",
    "\n",
    "chosen_actions = logts.fit_online(\n",
    "    X=X,\n",
    "    A_logged=A_logged,\n",
    "    R=R,\n",
    "    n_rounds=None\n",
    ")\n",
    "\n",
    "print(\"LogTS simulation finished.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Approximate pi_e(a_i | x_i) for all i using reduced Monte Carlo\n",
    "# ------------------------------------------------------------\n",
    "N = len(df)\n",
    "pi_e_logged = np.empty(N, dtype=float)\n",
    "\n",
    "for i in range(N):\n",
    "    probs = logts.action_probs_mc(X[i], n_mc=M_fast)\n",
    "    pi_e_logged[i] = probs[A_logged[i]]\n",
    "\n",
    "print(\"pi_e(a_i|x_i) estimated for all rows.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OPE with reduced bootstrap\n",
    "# ------------------------------------------------------------\n",
    "ope_results = ope_with_bootstrap(\n",
    "    pi_e=pi_e_logged,\n",
    "    pi_b=pi_b,\n",
    "    r=R,\n",
    "    B=B_fast,\n",
    "    seed=99\n",
    ")\n",
    "\n",
    "print(\"OPE results (reduced MC/bootstrap):\")\n",
    "print(ope_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0debfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation finished on small dataset.\n",
      "Policy probabilities estimated.\n",
      "Small-case OPE results:\n",
      "{'ipw': 0.008, 'ipw_ci95': (0.0, 0.022200000000000004), 'snipw': 0.027777777777777776, 'snipw_ci95': (0.0, 0.06306818181818183)}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Small-case sanity check (fast)\n",
    "# ------------------------------------------------------------\n",
    "A_logged = make_action_id(  # Construct logged action identifiers from item and position columns.\n",
    "    df[\"item_id\"].to_numpy(),  # Extract item IDs as a NumPy array.\n",
    "    df[\"position\"].to_numpy(),  # Extract positions as a NumPy array.\n",
    "    n_positions=3  # Specify the number of available positions.\n",
    ")\n",
    "\n",
    "R = df[\"click\"].to_numpy().astype(int)  # Extract logged rewards as a binary integer array.\n",
    "pi_b = df[\"propensity_score\"].to_numpy().astype(float)  # Extract behavior policy propensities for logged actions.\n",
    "\n",
    "\n",
    "N_small = 1000      # number of logged interactions\n",
    "M_small = 10        # Monte Carlo samples\n",
    "B_small = 10        # bootstrap resamples\n",
    "\n",
    "# Slice the data\n",
    "X_small = X[:N_small]\n",
    "A_logged_small = A_logged[:N_small]\n",
    "R_small = R[:N_small]\n",
    "pi_b_small = pi_b[:N_small]\n",
    "\n",
    "# Instantiate LogTS\n",
    "logts_small = LogTS(\n",
    "    n_actions=K,\n",
    "    context_dim=X.shape[1],\n",
    "    lam=1.0,\n",
    "    seed=123,\n",
    "    add_intercept=True\n",
    ")\n",
    "\n",
    "# Run online simulation\n",
    "chosen_actions_small = logts_small.fit_online(\n",
    "    X=X_small,\n",
    "    A_logged=A_logged_small,\n",
    "    R=R_small,\n",
    "    n_rounds=N_small\n",
    ")\n",
    "\n",
    "print(\"Simulation finished on small dataset.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Approximate pi_e(a_i | x_i) with small MC\n",
    "# ------------------------------------------------------------\n",
    "pi_e_logged_small = np.empty(N_small, dtype=float)\n",
    "\n",
    "for i in range(N_small):\n",
    "    probs = logts_small.action_probs_mc(X_small[i], n_mc=M_small)\n",
    "    pi_e_logged_small[i] = probs[A_logged_small[i]]\n",
    "\n",
    "print(\"Policy probabilities estimated.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OPE with small bootstrap\n",
    "# ------------------------------------------------------------\n",
    "ope_results_small = ope_with_bootstrap(\n",
    "    pi_e=pi_e_logged_small,\n",
    "    pi_b=pi_b_small,\n",
    "    r=R_small,\n",
    "    B=B_small,\n",
    "    seed=99\n",
    ")\n",
    "\n",
    "print(\"Small-case OPE results:\")\n",
    "print(ope_results_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c2bd64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m pi_e_logged \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(df), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# Allocate array for evaluation policy probability of the logged action.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):  \u001b[38;5;66;03m# Iterate over all logged interactions to compute pi_e(a_i|x_i).\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mlogts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_probs_mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Approximate the evaluation policy distribution over actions for context x_i.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     pi_e_logged[i] \u001b[38;5;241m=\u001b[39m probs[A_logged[i]]  \u001b[38;5;66;03m# Extract probability mass assigned to the logged action a_i.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m ope_results \u001b[38;5;241m=\u001b[39m ope_with_bootstrap(  \u001b[38;5;66;03m# Compute OPE estimates with bootstrap confidence intervals.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     pi_e\u001b[38;5;241m=\u001b[39mpi_e_logged,  \u001b[38;5;66;03m# Provide evaluation policy probability for the logged action per row.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     pi_b\u001b[38;5;241m=\u001b[39mpi_b,  \u001b[38;5;66;03m# Provide behavior policy propensity per row.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m  \u001b[38;5;66;03m# Set bootstrap RNG seed for reproducibility.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n",
      "Cell \u001b[0;32mIn[13], line 94\u001b[0m, in \u001b[0;36mLogTS.action_probs_mc\u001b[0;34m(self, x, n_mc)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_mc):  \u001b[38;5;66;03m# Repeat sampling to approximate selection probabilities.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     thetas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Sample one parameter vector per action.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthetas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_aug\u001b[49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# Select the action with highest sampled click probability.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     counts[a] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Increment selection count for the chosen action.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m counts \u001b[38;5;241m/\u001b[39m n_mc\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36msigmoid\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Utilities\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msigmoid\u001b[39m(z: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:  \u001b[38;5;66;03m# Define the logistic sigmoid function.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(z, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m35\u001b[39m)  \u001b[38;5;66;03m# Clip logits to prevent numerical overflow in exp().\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mz))  \u001b[38;5;66;03m# Compute elementwise sigmoid.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# End-to-end: prepare arrays, simulate LogTS, compute OPE\n",
    "# ------------------------------------------------------------\n",
    "A_logged = make_action_id(  # Construct logged action identifiers from item and position columns.\n",
    "    df[\"item_id\"].to_numpy(),  # Extract item IDs as a NumPy array.\n",
    "    df[\"position\"].to_numpy(),  # Extract positions as a NumPy array.\n",
    "    n_positions=3  # Specify the number of available positions.\n",
    ")\n",
    "R = df[\"click\"].to_numpy().astype(int)  # Extract logged rewards as a binary integer array.\n",
    "pi_b = df[\"propensity_score\"].to_numpy().astype(float)  # Extract behavior policy propensities for logged actions.\n",
    "\n",
    "K = 80 * 3  # Define the number of actions as item-position pairs.\n",
    "\n",
    "logts = LogTS(  # Instantiate a LogTS policy with hashed context inputs.\n",
    "    n_actions=K,  # Set the size of the action space.\n",
    "    context_dim=X.shape[1],  # Set the dimensionality of the hashed context vector.\n",
    "    lam=1.0,  # Set the initial diagonal precision (regularization strength).\n",
    "    seed=123,  # Set random seed for reproducible sampling.\n",
    "    add_intercept=True  # Enable an intercept feature for the logistic model.\n",
    ")\n",
    "\n",
    "chosen_actions = logts.fit_online(  # Simulate LogTS online over the logged contexts.\n",
    "    X=X,  # Provide the hashed context matrix.\n",
    "    A_logged=A_logged,  # Provide the logged action IDs.\n",
    "    R=R,  # Provide the logged rewards.\n",
    "    n_rounds=None  # Simulate the full dataset length.\n",
    ")\n",
    "\n",
    "n_mc = 200  # Specify the number of Monte Carlo samples for approximating pi_e(a|x).\n",
    "pi_e_logged = np.empty(len(df), dtype=float)  # Allocate array for evaluation policy probability of the logged action.\n",
    "\n",
    "for i in range(len(df)):  # Iterate over all logged interactions to compute pi_e(a_i|x_i).\n",
    "    probs = logts.action_probs_mc(X[i], n_mc=n_mc)  # Approximate the evaluation policy distribution over actions for context x_i.\n",
    "    pi_e_logged[i] = probs[A_logged[i]]  # Extract probability mass assigned to the logged action a_i.\n",
    "\n",
    "ope_results = ope_with_bootstrap(  # Compute OPE estimates with bootstrap confidence intervals.\n",
    "    pi_e=pi_e_logged,  # Provide evaluation policy probability for the logged action per row.\n",
    "    pi_b=pi_b,  # Provide behavior policy propensity per row.\n",
    "    r=R,  # Provide observed rewards.\n",
    "    B=100,  # Specify the number of bootstrap resamples.\n",
    "    seed=99  # Set bootstrap RNG seed for reproducibility.\n",
    ")\n",
    "\n",
    "print(ope_results)  # Print the resulting IPW and SNIPW estimates and confidence intervals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
